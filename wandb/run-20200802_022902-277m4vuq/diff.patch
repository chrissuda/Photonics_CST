diff --git a/main.py b/main.py
index a893364..6c051d6 100644
--- a/main.py
+++ b/main.py
@@ -1,9 +1,10 @@
 import torch
+from util import*
 from torchData import CSTData
 from model import CSTModel
 import wandb
 
-batch_size=64
+batch_size=256
 
 train_labelFile="data/train/train_label.json"
 val_labelFile="data/validation/val_label.json"
@@ -22,14 +23,15 @@ for param in model.parameters():
         param.requires_grad = True
         
 # Using GPU or CPU
-	if torch.cuda.is_available():
-		device = torch.device('cuda')
-		torch.backends.cuda.cufft_plan_cache.clear()
-	else:
-		device = torch.device('cpu')
+if torch.cuda.is_available():
+	device = torch.device('cuda')
+	torch.backends.cuda.cufft_plan_cache.clear()
+else:
+	device = torch.device('cpu')
 
 optimizer=torch.optim.Adam(model.parameters())
-criterion = nn.MSELoss()
-model=train(model,optimizer,trainloader,valloader,criterion,device,epochs=2)
+criterion = torch.nn.MSELoss()
+model=train(model,optimizer,trainloader,valloader,criterion,device,epochs=20)
 
-print("finish")
\ No newline at end of file
+print("finish")
+torch.save(model,"model.pt")
\ No newline at end of file
diff --git a/model.py b/model.py
index 426b07a..bd14c76 100644
--- a/model.py
+++ b/model.py
@@ -2,10 +2,12 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-#Process CST data 
+# Process CST data
+
+
 class CSTModel(nn.Module):
-    def __init__(self, input_size,hidden_size_1,hidden_size_2,hidden_size_3,num_classes):
-    super().__init__()
+    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, num_classes):
+        super().__init__()
         # assign layer objects to class attributes
         self.fc1 = nn.Linear(input_size, hidden_size_1)
         nn.init.kaiming_normal_(self.fc1.weight)
@@ -21,46 +23,51 @@ class CSTModel(nn.Module):
 
     def forward(self, x):
         # forward always defines connectivity
-        x=F.relu(self.fc1(x))
-        x=F.relu(self.fc2(x))
-        x=F.relu(self.fc3(x))
-        score=F.relu(self.fc4(x))
+        x = F.relu(self.fc1(x))
+        x = F.relu(self.fc2(x))
+        x = F.relu(self.fc3(x))
+        score = F.relu(self.fc4(x))
 
         return score
 
-#Process individual data with this model
+# Process individual data with this model
+
+
 class preModel(nn.Module):
-	def __init__(self, input_size, hidden_size, num_classes):
+    def __init__(self, input_size, hidden_size, num_classes):
         super().__init__()
         # assign layer objects to class attributes
         self.fc1 = nn.Linear(input_size, hidden_size)
         nn.init.kaiming_normal_(self.fc1.weight)
 
-        self.fc2 = nn.Linear(hidden_size,hidden_size)
+        self.fc2 = nn.Linear(hidden_size, hidden_size)
         nn.init.kaiming_normal_(self.fc2.weight)
 
-    	self.fc3=nn.Linear(hidden_size,num_classes)
-    	nn.init.kaiming_normal_(self.fc3.weight)
+        self.fc3 = nn.Linear(hidden_size, num_classes)
+        nn.init.kaiming_normal_(self.fc3.weight)
 
     def forward(self, x):
         # forward always defines connectivity
         x = torch.flatten(x)
-        x=F.relu(self.fc1(x))
-        x=F.relu(self.fc2(x))
-        scores=F.relu(self.fc3(x))
+        x = F.relu(self.fc1(x))
+        x = F.relu(self.fc2(x))
+        scores = F.relu(self.fc3(x))
         return scores
 
 
 class GPN(nn.Module):
-	def __init__(self, input_size_XPolar, hidden_size_XPolar, num_classes_XPolar,
-					   input_size_YPolar, hidden_size_YPolar, num_classes_YPolar,
-					   input_size_Properties, hidden_size_Properties, num_classes_Properties):
-	super().__init__()
+    def __init__(self, input_size_XPolar, hidden_size_XPolar, num_classes_XPolar,
+                    input_size_YPolar, hidden_size_YPolar, num_classes_YPolar,
+                    input_size_Properties, hidden_size_Properties, num_classes_Properties):
+        super().__init__()
         # assign layer objects to class attributes
 
-        self.Model_XPolar=preModel(input_size_XPolar,hidden_size_XPolar,num_classes_XPolar)
-        self.Model_YPolar=preModel(input_size_YPolar,hidden_size_YPolar,num_classes_YPolar)
-        self.Model_Properties=preModel(input_size_Properties,hidden_size_Properties,num_classes_Properties)
+        self.Model_XPolar = preModel(
+            input_size_XPolar, hidden_size_XPolar, num_classes_XPolar)
+        self.Model_YPolar = preModel(
+            input_size_YPolar, hidden_size_YPolar, num_classes_YPolar)
+        self.Model_Properties = preModel(
+            input_size_Properties, hidden_size_Properties, num_classes_Properties)
 
         self.fc1 = nn.Linear(input_size, hidden_size)
         nn.init.kaiming_normal_(self.fc1.weight)
@@ -106,7 +113,7 @@ class GPN(nn.Module):
         
 
 class SPN(nn.Module):
-	def __init__(self, input_size, hidden_size, num_classes):
+    def __init__(self, input_size, hidden_size, num_classes):
         super().__init__()
         # assign layer objects to class attributes
         self.fc1 = nn.Linear(input_size, hidden_size)
@@ -136,7 +143,7 @@ class SPN(nn.Module):
     def forward(self, x):
         # forward always defines connectivity
         x=torch.cat((x,x),0) #Create the input with 2xbatch_size
-        					#One for XPolarization and one for YPolarization's sepctrum
+        					# One for XPolarization and one for YPolarization's sepctrum
 
         x=F.relu(self.fc1(x))
         x=F.relu(self.fc2(x))
@@ -147,4 +154,4 @@ class SPN(nn.Module):
         x=F.relu(self.fc7(x))
         scores=F.relu(self.fc8(x))
 
-        return scores
\ No newline at end of file
+        return scores
diff --git a/torchData.py b/torchData.py
index 3785e50..9f96cd0 100644
--- a/torchData.py
+++ b/torchData.py
@@ -16,7 +16,7 @@ class CSTData(torch.utils.data.Dataset):
         x=[data["tet"],data["y"]]
         x=torch.as_tensor(x,dtype=torch.float32)  
 
-        y=[data["Shift"],data["swa"],data["swb"]]
+        y=[data["Shift"],data["swA"],data["swB"]]
         y=torch.as_tensor(y,dtype=torch.float32)   
 
         return x,y
diff --git a/util.py b/util.py
index e4ff7d6..d6defbd 100644
--- a/util.py
+++ b/util.py
@@ -35,11 +35,11 @@ def train(model,optimizer,trainloader,valloader,criterion,device,epochs=2):
 			optimizer.step()
 
 		#Evaluate the model after every epoch
-		val_loss,val_ShiftDiff,val_swaDiff,val_swbDiff=evaluate(model,valloader,criterion)
-		print("Epochs:",e," train_loss:",train_loss.data," val_loss:",val_loss,
+		val_loss,val_ShiftDiff,val_swADiff,val_swBDiff=evaluate(model,valloader,criterion,device)
+		print("Epochs:",e," train_loss:",train_loss.item()," val_loss:",val_loss,
 			" ShiftDiff:",val_ShiftDiff*100,"%",
-			" swaDiff:",val_swaDiff*100,"%",
-			" swbDiff:",val_swbDiff*100,"%")
+			" swADiff:",val_swADiff*100,"%",
+			" swBDiff:",val_swBDiff*100,"%")
 
 		#Log results to wandb
 		wandb.log({
@@ -47,35 +47,41 @@ def train(model,optimizer,trainloader,valloader,criterion,device,epochs=2):
 				"train_loss":train_loss,
 				"val_loss":val_loss,
 				"ShiftDiff":val_ShiftDiff,
-				"swaDiff:":val_swaDiff,
-				"swbDiff":val_swbDiff
+				"swADiff:":val_swADiff,
+				"swBDiff":val_swBDiff
 				})
 
 	return model
 
 
 def evaluate(model,loader,criterion,device):
+	loss,ShiftDiff,swADiff,swBDiff=0,0,0,0
 	lossList=[]
-	ShiftDiffList,swaDiffList,swbDiffList=[],[],[]
+	ShiftDiffList,swADiffList,swBDiffList=[],[],[]
 	
 	model = model.to(device=device)  # move the model parameters to CPU/GPU
 	model.eval()
 
-	with torch.no_grad:
-		for x,y in loader:
+	with torch.no_grad():
+		for i,(x, y) in enumerate(loader):
 			x=x.to(device=device) #move data to CPU/GPU
 			y=y.to(device=device)
 
 			target=model(x)
-			lossList.append(criterion(target,y).item())
-			ShiftDiffList.append(abs(target.data[0]-y.data[0])/y.data[0])
-			swaDiffList.append(abs(target.data[1]-y.data[1])/y.data[1])
-			swbDiffList.append(abs(target.data[2]-y.data[2])/y.data[2])
 
-	loss=round(mean(lossList),4)
-	ShiftDiffMean=round(mean(ShiftDiffList),4)
-	swaDiffMean=round(mean(swaDiffList),4)
-	swbDiffMean=round(mean(swbDiffList),4)
+			loss+=criterion(target,y).item()
+			ShiftDiff+=(torch.mean((torch.abs(target[:,0]-y[:,0])/y[:,0]))).item()
+			swADiff+=(torch.mean((torch.abs(target[:,1]-y[:,1])/y[:,1]))).item()
+			swBDiff+=(torch.mean((torch.abs(target[:,2]-y[:,2])/y[:,2]))).item()
 
-	return lossMean,ShiftDiffMean,swaDiffMean,swbDiffMean
+	#Update i
+	#i is 0 in the first loop, which suppose to be 1 instead.
+	i+=1 
+
+	loss=round(loss/i,4)
+	ShiftDiff=round(ShiftDiff/i,4)
+	swADiff=round(swADiff/i,4)
+	swBDiff=round(swBDiff/i,4)
+
+	return loss,ShiftDiff,swADiff,swBDiff
 
